# @package _global_

# to execute this experiment run:
# python train.py experiment=squadv2-multimodel_frozen_target_frozen_re trainer=gpu

defaults:
  # load everything from this config file
  - squadv2-multimodel_base.yaml

model:
  aggregate: attention
  pretrained_models:
    bert-base-cased-qa-squad2: ${local_pretrained_model_dir}/bert-base-cased-qa-squad2
    bert-base-cased-re-tacred: ${local_pretrained_model_dir}/bert-base-cased-re-tacred-20230919-hf
  pretrained_configs:
    # this is required because the RE model was trained with the code from this repo
    # and not with the Huggingface trainer
    bert-base-cased-re-tacred:
      name_or_path: "bert-base-cased"
      vocab_size: 29034
  freeze_models:
    - bert-base-cased-qa-squad2
    - bert-base-cased-re-tacred
