# @package _global_

# to execute this experiment run:
# python src/train.py experiment=conll2012_coref_hoi dataset.input.base_dataset_kwargs.data_dir=/absolute/path/to/seg_len_384

defaults:
  - override /dataset: conll2012_ontonotesv5_preprocessed.yaml
  - override /datamodule: default.yaml
  - override /taskmodule: coref_hoi.yaml
  - override /model: coref_hoi.yaml
  - override /callbacks: default.yaml
  - override /logger: aim.yaml
  - override /trainer: default.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

# name of the run determines folder name in logs
name: "conll2012/coref_hoi"

tags: ["dataset=conll2012", "model=coref_hoi"]

seed: 12345

# metric with mode (minimize or maximize) to monitor for checkpointing and early stopping callbacks
monitor_metric: "val/loss"
monitor_mode: "min"

taskmodule:
  # to work data preprocessed by Tatiana
  max_segment_len: 384
  # to work with model trained by Tatiana
  tokenizer_name_or_path: bert-base-cased
  max_training_sentences: 11

model:
  # taken from bert_base subset
  bert_pretrained_name_or_path: bert-base-cased
  #num_docs: 2802  # seems to be not used anywhere
  bert_learning_rate: 1e-05
  task_learning_rate: 2e-4
  ffnn_size: 3000
  cluster_ffnn_size: 3000
  #max_segment_len: 128  # passed from taskmodule
  #max_training_sentences: 11  # passed from taskmodule

trainer:
  min_epochs: 5
  max_epochs: 20
  gradient_clip_val: 1.0
  gradient_clip_algorithm: norm

datamodule:
  # the whole setup works only with batch_size=1!
  batch_size: 1
