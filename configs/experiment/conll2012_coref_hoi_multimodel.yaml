# @package _global_

# This implements a simple showcase for our idea.
# - task: coreference on conll2012
# - included pretrained models:
#   - ner trained on Ontonotes
#   - re trained on Tacred
# - aggregation: mean
#
# Prerequisites:
# - put the pretrained models to models/pretrained (see parameter model.model_paths below)
# - rename the model weight files (*.bin) to pytorch_model.bin, if not yet done
#
# To train, execute:
#
## fast dev run: on cpu, run just for two steps
# python src/train.py experiment=conll2012-coref-hoi-multimodel +trainer.fast_dev_run=true
#
## full training on gpu
# python src/train.py experiment=conll2012-coref-hoi-multimodel trainer=gpu
#
# The model implementation can be found here: src/models/multi_model_coref_hoi.py
#
# The config is here: configs/experiments/conll2012_coref_hoi_multimodel.yaml (this file)

defaults:
  # load everything from this config file
  - _conll2012_coref_hoi_multimodel.yaml

model:
  # This should be a mapping from an arbitrary model identifier to a pretrained model name or path
  # that can be loaded with Huggingface AutoModel.from_pretrained.
  pretrained_models:
    bert-base-cased-ner-ontonotes: "models/pretrained/bert-base-cased-ner-ontonotes"
    bert-base-cased-re-tacred: "models/pretrained/bert-base-cased-re-tacred"
    bert-base-cased-qa-squad2: "models/pretrained/bert-base-cased-qa-squad2"
    bert-base-cased: "bert-base-cased"
