# @package _global_

# This implements a simple showcase for our idea.
# - task: RE on Tacred
# - included pretrained models:
#   - ner trained on Ontonotes
#   # - re trained on Tacred
#   - coreference trained on conll2012
# - aggregation: mean
#
# Prerequisites:
# - put the pretrained models to models/pretrained (see parameter model.model_paths below)
# - rename the model weight files (*.bin) to pytorch_model.bin, if not yet done
#
# To train, execute:
#
## fast dev run: on cpu, run just for two steps
# python src/train.py experiment=tacred-multimodel +trainer.fast_dev_run=true
#
## full training on gpu
# python src/train.py experiment=tacred-multimodel trainer=gpu
#
# The model implementation can be found here: src/models/multi_model_text_classification.py
#
# The config is here: configs/experiments/tacred-multimodel.yaml (this file)

defaults:
  - tacred-multimodel_base.yaml

model:
  pretrained_models:
    bert-base-cased-re-tacred: "models/pretrained/bert-base-cased-re-tacred"
    bert-base-cased-ner-ontonotes:
      name_or_path: "models/pretrained/bert-base-cased-ner-ontonotes"
      vocab_size: 28996
    coreference:
      name_or_path: "models/pretrained/coreference"
      vocab_size: 28996
  # freeze_models:
  #  - bert-base-cased-ner-ontonotes
